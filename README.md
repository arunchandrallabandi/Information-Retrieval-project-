## Project Overview
This project is a simple information retrieval system built around a TF‑IDF search index and a Flask web interface.​
It crawls HTML pages, builds a TF‑IDF representation of their visible text, and exposes a search UI with snippet highlighting and auto‑suggestions.​

## Features
TF‑IDF indexing of crawled HTML pages using scikit‑learn, with stopword removal via NLTK.​

Web search interface in Flask supporting ranked results, highlighted snippets, “Read More” links, and auto‑suggestions.​

## Requirements
Python 3.9+ (recommended).

Python packages: scrapy, flask, scikit-learn, nltk, numpy, beautifulsoup4, requests.​

NLTK stopwords corpus (downloaded at index time by indexer.py).​

You can install typical dependencies with:

bash
pip install scrapy flask scikit-learn nltk numpy beautifulsoup4 requests
Then, in a Python shell (once), download NLTK stopwords:

python
import nltk
nltk.download("stopwords")

## Project Structure
crawler/ (expected): Scrapy project containing the document_spider Spider that stores crawled HTML pages under crawler/crawler/pages/.​

indexer.py: Reads crawled pages, builds a TF‑IDF model, and serializes index artifacts.​

query.py: Loads TF‑IDF data and implements search and suggest utilities (ranking, snippets, suggestions).​

app.py: Flask app that exposes the search web UI and suggestion API, serving original HTML pages from the pages directory.​

test_ir_sytem.py: Script of functional tests that exercise the running server (search, snippets, suggestions, links, multiple queries, rare terms).​

## Data and Index Files
By default, the system expects crawled HTML pages in crawler/crawler/pages/ and uses those to build the index.​
The indexer script creates the following serialized artifacts in the project directory:

tfidf_vectorizer.pkl: Pickled TfidfVectorizer fitted on the documents.​

tfidf_matrix.pkl (or similar): Pickled sparse TF‑IDF matrix with one row per document.​

filenames.pkl: List of filenames corresponding to rows of the matrix.​

query.py loads these files at import time so they must exist before starting the server.​

## Running the Pipeline
## 1. Launch the Crawler
Use Scrapy to crawl documents and store results:

bash

scrapy crawl document_spider -O crawled_data.json

document_spider should be defined in your Scrapy project and configured to save HTML pages into crawler/crawler/pages/ (as used by indexer.py and app.py).​

The -O crawled_data.json flag overwrites the JSON file on each run with structured results such as URL, title, and cleaned text.

## 2. Build the Search Index
Once pages are available under crawler/crawler/pages/, build the TF‑IDF index:

bash
python indexer.py
indexer.py scans the pages directory, reads each file, and collects their contents into a document list.​

It fits a TfidfVectorizer (with English stopwords) and serializes the vectorizer, matrix, and filenames into .pkl files for later use.​

## 3. Start the Query Server
After the index is built, launch the Flask web application:

bash
python app.py
The app creates a Flask server and loads search and suggest from query.py, which in turn load the TF‑IDF artifacts.​

By default the app runs in debug mode and listens on http://127.0.0.1:5000.​

## 4. Using the Web Interface
Navigate to http://127.0.0.1:5000 in your browser.

Enter a query in the search form and submit; the backend calls search(query, top_k) which ranks documents by cosine similarity between the query and the TF‑IDF matrix.​

Results show snippets where query terms are highlighted using <mark> tags, generated by the snippet function in query.py.​

To view full pages, use the “Read More” links:

app.py exposes a /page/<path:filename> route that serves original HTML pages from crawler/crawler/pages/ via send_from_directory.​

For auto-suggestions:

The /suggest endpoint accepts a q query parameter and returns JSON suggestions based on partial matches in document text.​

Frontend JavaScript can call /suggest?q=<prefix> to show live suggestions as the user types.

## Running Tests
Ensure the Flask app is running on http://127.0.0.1:5000 before running tests.​
Then, in another terminal, run:

bash
python test_ir_sytem.py
The script issues HTTP requests to the server (search POSTs, suggestion GETs, page GETs) using requests.​

It checks search endpoint availability, snippet highlighting (<mark> tags), auto‑suggestion behavior, “Read More” link validity, multiple sequential queries, and rare query term handling, printing a simple test report to stdout.​
